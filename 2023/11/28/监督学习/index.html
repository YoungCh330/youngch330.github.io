<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"youngch330.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.18.0","exturl":false,"sidebar":{"position":"left","display":"remove","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="线性回归 线性回归函数 线性回归函数如下，其中w代表斜率，b代表截距； \[ f_{w,b}(x^{(i)}) &#x3D; wx^{(i)} + b \] 成本函数 \[J(w,b) &#x3D; \frac{1}{2m} \sum\limits_{i &#x3D; 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \] - 成本函数的关键是找到一个w和b，使得成本函数在训练集上的值最小或接近最">
<meta property="og:type" content="article">
<meta property="og:title" content="监督学习">
<meta property="og:url" content="https://youngch330.github.io/2023/11/28/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="Young&#39;s 博客">
<meta property="og:description" content="线性回归 线性回归函数 线性回归函数如下，其中w代表斜率，b代表截距； \[ f_{w,b}(x^{(i)}) &#x3D; wx^{(i)} + b \] 成本函数 \[J(w,b) &#x3D; \frac{1}{2m} \sum\limits_{i &#x3D; 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \] - 成本函数的关键是找到一个w和b，使得成本函数在训练集上的值最小或接近最">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2023-11-28T14:40:26.000Z">
<meta property="article:modified_time" content="2023-11-28T14:43:15.620Z">
<meta property="article:author" content="Young">
<meta property="article:tag" content="Java,Python,算法,读书笔记">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://youngch330.github.io/2023/11/28/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://youngch330.github.io/2023/11/28/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/","path":"2023/11/28/监督学习/","title":"监督学习"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>监督学习 | Young's 博客</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Young's 博客</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">心之所向，行必能至</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section">首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section">标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section">分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section">归档</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section">关于</a></li>
  </ul>
</nav>




</header>
    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://youngch330.github.io/2023/11/28/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Young">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Young's 博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="监督学习 | Young's 博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          监督学习
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2023-11-28 22:40:26 / 修改时间：22:43:15" itemprop="dateCreated datePublished" datetime="2023-11-28T22:40:26+08:00">2023-11-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="线性回归">线性回归</h2>
<h3 id="线性回归函数">线性回归函数</h3>
<p>线性回归函数如下，其中w代表斜率，b代表截距； <span
class="math display">\[ f_{w,b}(x^{(i)}) = wx^{(i)} + b \]</span></p>
<h3 id="成本函数">成本函数</h3>
<p><span class="math display">\[J(w,b) = \frac{1}{2m} \sum\limits_{i =
0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \]</span> -
成本函数的关键是找到一个w和b，使得成本函数在训练集上的值最小或接近最小；
- 其中<span
class="math inline">\(f_{w,b}(x^{(i)})\)</span>为预测值，<span
class="math inline">\((f_{w,b}(x^{(i)})
-y^{(i)})^2\)</span>为预测值和目标值之差的平方；</p>
<h3 id="梯度下降算法">梯度下降算法</h3>
<p>梯度下降算法如下： <span class="math display">\[\begin{align*}
\text{repeat}&amp;\text{ until convergence:} \; \lbrace \newline
\;  w &amp;= w -  \alpha \frac{\partial J(w,b)}{\partial w} \; \newline
b &amp;= b -  \alpha \frac{\partial J(w,b)}{\partial b}  \newline
\rbrace
\end{align*}\]</span> 当参数w和b变化时，梯度算法实际上是求偏导数: <span
class="math display">\[
\begin{align}
\frac{\partial J(w,b)}{\partial w}  &amp;= \frac{1}{m} \sum\limits_{i =
0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \;\newline
\frac{\partial J(w,b)}{\partial b}  &amp;= \frac{1}{m} \sum\limits_{i =
0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \newline
\end{align}
\]</span></p>
<h3 id="特征缩放">特征缩放</h3>
<ol type="1">
<li>原始特征除特征值的最大值，将特征值缩放到[0,1];</li>
<li>均值归一化：计算特征的均值μ，通过公式<span
class="math inline">\(\frac{x-μ}{max-min}\)</span>将特征值调整到[-1,1]之间；</li>
<li>z-score标准化：计算特征的标准差σ和均值μ，通过公式<span
class="math inline">\(\frac{x-μ}{σ}\)</span>进行特征缩放；</li>
</ol>
<h3 id="通过向量实现线性回归">通过向量实现线性回归</h3>
<ol type="1">
<li>实现成本函数 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cost</span>(<span class="params">x,y,w,b</span>):</span><br><span class="line">    cost = np.<span class="built_in">sum</span>(((w * x + b) - y) ** <span class="number">2</span>) / (<span class="number">2</span> * m)</span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure></li>
<li>计算每次下降的梯度 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_gradient</span>(<span class="params">x,y,w,b</span>):</span><br><span class="line">    dj_dw = np.<span class="built_in">sum</span>((w * x + b - y) * x) / m </span><br><span class="line">    dj_db = np.<span class="built_in">sum</span>((w * x) +b -y) / m</span><br><span class="line">    <span class="keyword">return</span> dj_dw,dj_db</span><br></pre></td></tr></table></figure></li>
<li>实现梯度下降算法,此时可获得最佳w,b进行预测 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">x, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters</span>): </span><br><span class="line">    j_history = []</span><br><span class="line">    w = copy.deepcopy(w_in)</span><br><span class="line">    b = b_in</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line">        dj_dw,dj_db = compute_gradient(x,y,w,b)</span><br><span class="line">        w = w - alpha * dj_dw</span><br><span class="line">        b = b - alpha * dj_db</span><br><span class="line">        j_history.append(compute_cost(x,y,w,b))</span><br><span class="line">    <span class="keyword">return</span> w,b,j_history</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="使用scikit-learn实现线性回归">使用scikit-learn实现线性回归</h3>
<ol type="1">
<li>加载数据</li>
<li>归一化处理：<code>StandardScaler.fit_transform(X_train)</code></li>
<li>拟合回归模型，获取参数w和b <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sgdr = SGDRegressor(max_iter=<span class="number">1000</span>)</span><br><span class="line">sgdr.fit(X_norm, y_train)</span><br><span class="line">b_norm = sgdr.intercept_</span><br><span class="line">w_norm = sgdr.coef_</span><br></pre></td></tr></table></figure></li>
<li>执行预测 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sgdr.predict(X_norm)</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="逻辑回归">逻辑回归</h2>
<h3 id="逻辑回归函数">逻辑回归函数</h3>
<p>z是线性回归模型的输出结果，g(z)代表对于给定的输入，结果为1的概率；
<span class="math display">\[g(z) = \frac{1}{1+e^{-z}}\]</span> <span
class="math display">\[z=wx+b\]</span></p>
<h3 id="损失函数">损失函数</h3>
<ol type="1">
<li><p>损失是单个示例与其目标值的差异度量，而成本是训练集损失的度量；</p></li>
<li><p>损失函数如下： <span class="math display">\[
\begin{equation*}
  loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)}) = \begin{cases}
- \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)
&amp; \text{if $y^{(i)}=1$}\\
- \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right)
\right) &amp; \text{if $y^{(i)}=0$}
  \end{cases}
\end{equation*}
\]</span></p></li>
</ol>
<ul>
<li><span
class="math inline">\(f_{\mathbf{w},b}(\mathbf{x}^{(i)})\)</span>是模型的预测，<span
class="math inline">\(y^{(i)}\)</span>是目标值；</li>
<li><span class="math inline">\(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) =
g(\mathbf{w} \cdot\mathbf{x}^{(i)}+b)\)</span> 函数<span
class="math inline">\(g\)</span>是sigmoid函数；</li>
</ul>
<ol start="3" type="1">
<li><p>成本函数如下： <span class="math display">\[J(w,b) = \frac{1}{m}
\sum\limits_{i = 1}^{m} (loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}),
y^{(i)})) \]</span></p></li>
<li><p>简化的损失函数： <span
class="math display">\[loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)})
= -y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right)
\right) - \left( 1 - y^{(i)}\right) \log \left( 1 -
f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right)
\right)\]</span></p></li>
</ol>
<h3 id="梯度下降">梯度下降</h3>
<p>梯度下降算法如下： <span class="math display">\[\begin{align*}
\text{repeat}&amp;\text{ until convergence:} \; \lbrace \newline
\;  w &amp;= w -  \alpha \frac{\partial J(w,b)}{\partial w} \; \newline
b &amp;= b -  \alpha \frac{\partial J(w,b)}{\partial b}  \newline
\rbrace
\end{align*}\]</span> 当参数w和b变化时，梯度算法实际上是求偏导数: <span
class="math display">\[
\begin{align*}
\frac{\partial J(w,b)}{\partial w}  &amp;= \frac{1}{m} \sum\limits_{i =
0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \;\newline
\frac{\partial J(w,b)}{\partial b}  &amp;= \frac{1}{m} \sum\limits_{i =
0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \newline
\end{align*}
\]</span></p>
<h3 id="使用scikit-learn实现逻辑回归">使用scikit-learn实现逻辑回归</h3>
<ol type="1">
<li><p>拟合模型 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lr_model = LogisticRegression()</span><br><span class="line">lr_model.fit(X, y)</span><br></pre></td></tr></table></figure></p></li>
<li><p>进行预测 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_pred = lr_model.predict(X)</span><br></pre></td></tr></table></figure></p></li>
</ol>
<h2 id="过拟合">过拟合</h2>
<h3 id="什么是过拟合">什么是过拟合</h3>
<p>完美适应训练集数据，但是在其他数据集上表现不佳；</p>
<h3 id="如何解决过拟合">如何解决过拟合</h3>
<ol type="1">
<li>收集更多的数据放入训练集中；</li>
<li>选择使用最相关的特征，当你有过多的特征但训练集数据不足时容易出现过拟合；</li>
<li>通过正则化减少参数的大小，相对特征选择是一种更温和的方式；</li>
</ol>
<h3 id="正则化">正则化</h3>
<ol type="1">
<li>通过差的平方和可避免模型与训练集拟合度不够，通过w的平方和可避免w过大产生的过拟合，特别是高阶多项式的过拟合问题；</li>
<li>思考当<span
class="math inline">\(\lambda\)</span>等于0或者等于无线大时，成本函数的含义是什么？</li>
</ol>
<h4 id="线性回归的正则化">线性回归的正则化</h4>
<p>正则化后的成本函数如下 <span class="math display">\[J(\mathbf{w},b) =
\frac{1}{2m} \sum\limits_{i = 0}^{m-1}
(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})^2  +
\frac{\lambda}{2m}  \sum_{j=0}^{n-1} w_j^2\]</span></p>
<h4 id="逻辑回归的正则化">逻辑回归的正则化</h4>
<p>正则化后的成本函数如下 <span class="math display">\[J(\mathbf{w},b) =
\frac{1}{m}  \sum_{i=0}^{m-1} \left[ -y^{(i)}
\log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) -
\left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left(
\mathbf{x}^{(i)} \right) \right) \right] +
\frac{\lambda}{2m}  \sum_{j=0}^{n-1} w_j^2\]</span></p>
<h4 id="梯度下降-1">梯度下降</h4>
<p><span class="math display">\[\begin{align*}
\frac{\partial J(\mathbf{w},b)}{\partial w_j}  &amp;= \frac{1}{m}
\sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) -
y^{(i)})x_{j}^{(i)}  +  \frac{\lambda}{m} w_j\\
\frac{\partial J(\mathbf{w},b)}{\partial b}  &amp;= \frac{1}{m}
\sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})
\end{align*}\]</span></p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/11/28/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="prev" title="神经网络">
                  <i class="fa fa-angle-left"></i> 神经网络
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2023</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Young</span>
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
